\relax 
\bibstyle{apalike}
\citation{Giacomantonio2010}
\citation{Hinton1987}
\citation{Waddington2012}
\citation{Jaeger2014}
\citation{Kauffman1995}
\citation{Bornholdt2008}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{Krubitzer2013}
\citation{Krubitzer2012}
\citation{Krubitzer2012}
\citation{Karbowski2004}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Cortical Arealisation}{2}}
\citation{Kauffman1969}
\citation{Bak1993}
\citation{Bornholdt2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Boolean Networks}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \emph  {Varying cortical areas in mammals}. Source: \citealp  {Krubitzer2012}\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mammals}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \emph  {Morphogen gradients}. The expression levels of five proteins where a darker shade represents a higher concentration. These gradients provide a co-ordinate system which guides axons from the thalamus to the cortex, creating boundaried areas.\relax }}{5}}
\newlabel{fig:gradients}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \emph  {A feasible network}. The figure represents the activating ($\rightarrow $) or inactivating ($\dashv $) interactions between five genes. This particular network is only demonstrative, the actual interactions responsible for cortical arealisation are unknown.\relax }}{6}}
\newlabel{fig:network}{{3}{6}}
\citation{Giacomantonio2010}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \emph  {The dynamics of a random three-node boolean network}. Each black dot represents a state which is labelled by the activation levels of the network's three nodes. Each state transitions deterministically, creating two basins of attraction, on the left is a `point attractor', on the right is a `limit cycle'.\relax }}{9}}
\newlabel{fig:statespace}{{4}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \emph  {Attractor diagrams for 4 nearby networks}. The original network is the one visualised in figure 4\hbox {}, where the state 001 transitions to 011, in networks a,b and c the state 001 transitions to 111, 001, and 010 respectively. So each of these networks have only one bit different from the original in their truth tables. The black markers shows states that are point attractors, the red markers show states that are in limit cycles.\relax }}{10}}
\newlabel{fig:close}{{5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Fitness Landscapes}{11}}
\citation{Alon2007}
\@writefile{toc}{\contentsline {section}{\numberline {2}Models}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \emph  {Possible interactions for our networks}. The space of all potential interactions is massively constrained to the smaller set in this figure. The interactions shaded green are the ones to be trialed in all possible combinations, where as the non-shaded interactions are fixed for all networks. Each `+' interaction is included with an AND operator, each `-' involves a NOT interaction.\relax }}{13}}
\newlabel{fig:table1}{{6}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Synchronous or Asynchronous?}{14}}
\citation{Kauffman1969}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Points of comparison}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Models for evolutionary dynamics}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results - Evaluation of deterministic model}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Proportion of networks that are successful is 0.01\%}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The best networks from the original model were not successful}{18}}
\newlabel{fig:best-a}{{7a}{19}}
\newlabel{sub@fig:best-a}{{(a)}{a}}
\newlabel{fig:best-b}{{7b}{19}}
\newlabel{sub@fig:best-b}{{(b)}{b}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{19}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{19}}
\newlabel{fig:success}{{7c}{20}}
\newlabel{sub@fig:success}{{(c)}{c}}
\newlabel{fig:random}{{7d}{20}}
\newlabel{sub@fig:random}{{(d)}{d}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \emph  {State spaces of four networks from our model}. Each figure depicts the dynamics of a different network, with each node representing a particular state which leads deterministically to another state, and eventually to a limit-cycle or point attractor. The states in the limit-cycle are labelled with the decimal conversion of their binary state. The triangles mark the location of the desired final state red for anterior (693), blue for posterior (330), the stars mark the initial states, with the same colouring. \textbf  {(a)} and \textbf  {(b)} show the best networks from the Goodhill model, they both have the correct attractors but the posterior initial state will not develop correctly. \textbf  {(c)} shows a successful network. \textbf  {(d)} shows a random unsuccessful network for comparison. The code to reproduce these figures can be found in `best2.py'.\relax }}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{20}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The frequencies of particular interactions are almost identical between models}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}The state space dynamics have some smoothness over networks}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results - Structure of fitness landscape}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Comparison of null models}{21}}
\newlabel{fig:freq-table}{{8a}{22}}
\newlabel{sub@fig:freq-table}{{(a)}{a}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{22}}
\newlabel{fig:freq-correlation}{{8b}{23}}
\newlabel{sub@fig:freq-correlation}{{(b)}{b}}
\newlabel{fig:freq-dist}{{8c}{23}}
\newlabel{sub@fig:freq-dist}{{(c)}{c}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \emph  {Frequency of interactions within successful networks}.\textbf  {(a)} orders the interactions by how frequent they are in successful networks in the original model (GG2010). The interaction marked * is fixed in all networks and therefore must occur in 100\% of cases. The frequencies given by our model are remarkably similar. \textbf  {(b)} plots the frequency of each interaction in both models with the line y = x showing a perfect correspondence. \textbf  {(c)} has each x value as one of the 25 interactions, showing how close each models frequency distribution is.\relax }}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{23}}
\newlabel{fig:attractors}{{9a}{24}}
\newlabel{sub@fig:attractors}{{(a)}{a}}
\newlabel{fig:attractors-random}{{9b}{24}}
\newlabel{sub@fig:attractors-random}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \emph  {Attractor locations and type}.These figures plot the limit cycles and point attractors for \textbf  {(a)} all successful networks and \textbf  {(b)} the same number of random networks. The x-values are the limit-cycle states encoded as a decimal, the y-axis contains each network in order of its Hamming distance from a reference successful network. The black markers are point attractors, the red markers are states within limit-cycles. Some level of smoothness can be seen in figure \textbf  {(a)} by the similarity of dynamics with distance from the reference network. The code to reproduce these plots is in `attractors.py'.\relax }}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{24}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Our fitness function gives smooth landscape}{25}}
\newlabel{fig:rough}{{10a}{26}}
\newlabel{sub@fig:rough}{{(a)}{a}}
\newlabel{fig:roughslopes}{{10b}{26}}
\newlabel{sub@fig:roughslopes}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \emph  {Distribution of generations taken to find maximum fitness - rough landscape}. Each generation the genome's fitness is measured, in this case it is a random number. Each bit in the genome is flipped with probability p. One trial consists of as many generations needed until the fitness has increased to its maximum. \textbf  {(a)} shows the frequency of different trial lengths, in this rough landscape the value of p is irrelevant. \textbf  {(b)} shows the gradients from \textbf  {(a)}, a steeper gradient would mean the peak fitness is found faster, but in this case there is no correlation between p and rate of finding the peak. The code to reproduce these figures can be found in 'evolve.py'.\relax }}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{26}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{26}}
\newlabel{fig:smooth}{{11a}{27}}
\newlabel{sub@fig:smooth}{{(a)}{a}}
\newlabel{fig:smoothslopes}{{11b}{27}}
\newlabel{sub@fig:smoothslopes}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \emph  {Distribution of generations taken to find maximum fitness - smooth landscape}. \textbf  {(a)} In a perfectly smooth landscape the algorithm finds the peak much faster for smaller values of p. In this case lower p values have steeper gradients \textbf  {(b)}, as they use the smoothness to approach the peak fitness faster. The code to reproduce these figures can be found in 'evolve.py'.\relax }}{27}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{27}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evolutionary search is up to 39 times faster than undirected search}{28}}
\newlabel{fig:f1}{{12a}{29}}
\newlabel{sub@fig:f1}{{(a)}{a}}
\newlabel{fig:f1slopes}{{12b}{29}}
\newlabel{sub@fig:f1slopes}{{(b)}{b}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \emph  {Distribution of generations taken to find maximum fitness - for our fitness landscape}. Our fitness function gives results similar to the smooth null model shown in figure 11a\hbox {}. The code to reproduce these figures can be found in 'evolve.py'.\relax }}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{29}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \emph  {Values of fitness surrounding peaks}.Another indication of the smoothness in our landscape is found by counting how many surrounding networks have non-zero fitness. As we look further from the peak the proportion decreases exponentially. The code to reproduce this figure is in 'peak.py'\relax }}{30}}
\newlabel{fig:peak}{{13}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}A deterministic boolean network for cortical arealisation}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \emph  {Conjunction of successful sets between models}. A venn diagram showing that there must be some conjunction between sets of successful networks from the two models (at least 579 networks but likely higher). The size of the largest circle is not to scale but the smaller circles' areas represent the number of networks in them. From largest to smallest the number of networks is $2^{24}, 6980, 5849, 1710$.\relax }}{32}}
\newlabel{fig:venn}{{14}{32}}
\citation{Harvey1997}
\citation{Hallinan2004}
\citation{Alon2007}
\citation{Koonin2011}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \emph  {Distribution of inputs per node}. It is common for real biological networks to have a decreasing power-law distribution for the number of interactions per node. This would give a straight line on this graph with a negative slope. The points plotted here are the frequency of inputs per node for all of our successful networks, where the power-law distribution does not hold, although there is a decrease in frequency for 2 to 4 interactions. The code in `scaling.py' will reproduce this figure.\relax }}{34}}
\newlabel{fig:scaling}{{15}{34}}
\citation{Hinton1987}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Development accelerating evolution}{35}}
\bibdata{references}
\bibcite{Alon2007}{{1}{2007}{{Alon}}{{}}}
\bibcite{Bak1993}{{2}{1993}{{Bak and Sneppen}}{{}}}
\bibcite{Bornholdt2008}{{3}{2008}{{Bornholdt}}{{}}}
\bibcite{Giacomantonio2010}{{4}{2010}{{Giacomantonio and Goodhill}}{{}}}
\bibcite{Hallinan2004}{{5}{2004}{{Hallinan et~al.}}{{}}}
\bibcite{Harvey1997}{{6}{1997}{{Harvey and Bossomaier}}{{}}}
\bibcite{Hinton1987}{{7}{1987}{{Hinton and Nowlan}}{{}}}
\bibcite{Jaeger2014}{{8}{2014}{{Jaeger and Monk}}{{}}}
\bibcite{Karbowski2004}{{9}{2004}{{Karbowski and Ermentrout}}{{}}}
\bibcite{Kauffman1995}{{10}{1995}{{Kauffman}}{{}}}
\bibcite{Kauffman1969}{{11}{1969}{{Kauffman}}{{}}}
\bibcite{Koonin2011}{{12}{2011}{{Koonin}}{{}}}
\bibcite{Krubitzer2013}{{13}{2013}{{Krubitzer and Dooley}}{{}}}
\bibcite{Krubitzer2012}{{14}{2012}{{Krubitzer and Seelke}}{{}}}
\bibcite{Waddington2012}{{15}{2012}{{Waddington}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Supporting Information}{39}}
